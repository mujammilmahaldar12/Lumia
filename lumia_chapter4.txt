CHAPTER 4  SYSTEM DESIGN  
	4.1 Architecture Overview  
	4.2 Module Layering  
	4.3 Data Model  
	4.4 Process Flows  
	4.5 Interaction Sequences  
	4.6 API Principles  
	4.7 Deployment Context  
	4.8 Configuration Management  

CHAPTER 4: SYSTEM DESIGN

This chapter presents the comprehensive system design for the Lumia AI Financial Analytics Platform, encompassing architectural patterns, module organization, data modeling, process workflows, and deployment strategies. The design follows modern software engineering principles, emphasizing scalability, maintainability, and performance optimization for financial data processing applications.

The system architecture adopts a layered approach with clear separation of concerns, enabling modular development, testing, and deployment. The design accommodates current requirements while providing extensibility for future enhancements including real-time sentiment analysis, advanced portfolio optimization, and machine learning integration.

4.1 Architecture Overview

4.1.1 High-Level System Architecture

The Lumia platform implements a multi-tier architecture combining microservices principles with monolithic deployment for initial simplicity. The architecture consists of five primary layers:

**Presentation Layer (Frontend)**
- Streamlit-based web application providing user interfaces
- Interactive dashboards for portfolio visualization and analysis
- Real-time data updates and responsive user interactions
- Input forms for investment preference specification

**API Layer (Application Interface)**
- FastAPI-based REST services for all system interactions
- OpenAPI 3.0 compliant documentation and validation
- Authentication and authorization middleware
- Request/response transformation and error handling

**Business Logic Layer (Core Services)**
- Recommendation engine with algorithmic portfolio optimization
- Signal generation services for technical and fundamental analysis
- Sentiment analysis pipeline for news and social media processing
- Risk assessment and compliance validation modules

**Data Access Layer (Persistence)**
- SQLAlchemy ORM for database abstraction and relationship management
- Repository pattern implementation for data access optimization
- Connection pooling and transaction management
- Query optimization and caching strategies

**Data Storage Layer (Infrastructure)**
- PostgreSQL with TimescaleDB extension for time-series optimization
- Hypertable partitioning for efficient financial data storage
- Automated compression and retention policies
- Backup and disaster recovery mechanisms

4.1.2 System Integration Architecture

**External Data Integration**
The platform integrates with multiple external data sources through standardized API interfaces:

- **Market Data Providers**: Real-time and historical price feeds
- **News APIs**: Financial news aggregation from multiple sources
- **Social Media APIs**: X (Twitter) and Reddit for sentiment analysis
- **Economic Data Sources**: Federal Reserve and statistical agencies

**Internal Service Communication**
Microservices communicate through well-defined interfaces:

- **Synchronous Communication**: HTTP/REST APIs for real-time requests
- **Asynchronous Processing**: APScheduler for batch operations and data collection
- **Event-Driven Architecture**: Planned for future real-time processing requirements
- **Database Sharing**: Controlled access through service-specific schemas

4.1.3 Scalability and Performance Architecture

**Horizontal Scaling Strategy**
- Stateless API design enabling load balancer distribution
- Database read replicas for analytical workloads
- Microservice decomposition for independent scaling
- Container-ready architecture for cloud deployment

**Performance Optimization**
- Database indexing strategy optimized for financial queries
- Connection pooling and query result caching
- Asynchronous processing for non-blocking operations
- TimescaleDB compression for historical data efficiency

4.2 Module Layering

4.2.1 Presentation Layer Modules

**Frontend Application Module (`app/`)**
```
app/
├── main.py              # Streamlit application entry point
├── test_ui.py           # UI testing and validation components
├── routes/              # API route definitions and handlers
│   ├── __init__.py
│   └── recommend.py     # Recommendation endpoint implementations
└── services/            # Business logic service integrations
    ├── __init__.py
    ├── news_collector.py    # News data collection services
    ├── scheduler.py         # Task scheduling and automation
    ├── sentiment_worker.py  # Sentiment analysis processing
    └── signal_generator.py  # Technical signal calculations
```

**User Interface Components**
- **Dashboard Module**: Portfolio overview and performance metrics visualization
- **Recommendation Module**: Interactive recommendation generation and display
- **Analytics Module**: Market analysis and comparative performance tools
- **Configuration Module**: User preference management and system settings

4.2.2 Business Logic Layer Modules

**Core Services Architecture**
```
services/
├── recommendation_engine.py    # Portfolio optimization algorithms
├── signal_processing.py       # Technical indicator calculations
├── sentiment_analysis.py      # News and social media processing  
├── risk_assessment.py         # Portfolio risk evaluation
├── portfolio_optimization.py  # Modern portfolio theory implementation
└── market_analysis.py         # Comparative analysis and benchmarking
```

**Data Collection Modules (`collectors/`)**
```
collectors/
├── __init__.py
├── company_collector.py        # Company fundamental data collection
├── daily_price_collector.py    # Market price data ingestion
├── quarterly_collector.py      # Financial statement data processing
├── unified_financial_collector.py  # Multi-source data aggregation
└── universal_collector.py      # Generic data collection framework
```

**News and Sentiment Processing (`news_collector/`)**
```
news_collector/
├── __init__.py
├── base.py                     # Base collector interface
├── algo_trading_collector.py   # Algorithmic trading news sources
├── company_website_collector.py # Direct company information
├── sentiment_analyzer.py       # Multi-model sentiment analysis
├── social_media_collector.py   # X/Twitter and Reddit integration
├── stock_research_collector.py # Professional research aggregation
└── unified_pipeline.py         # Coordinated data processing pipeline
```

4.2.3 Data Access Layer Modules

**Database Models (`models/`)**
```
models/
├── __init__.py
├── assets.py               # Asset classification and metadata
├── daily_price.py          # Time-series price data
├── asset_daily_signals.py  # Calculated technical indicators
├── news_article.py         # News content and metadata
├── news_sentiment.py       # Sentiment analysis results
├── news_asset_map.py       # Article-to-asset relationships
├── user.py                 # User profiles and preferences
├── quarterly_fundamental.py # Fundamental analysis data
└── company.py              # Company-specific information
```

**Specialized Asset Models**
- **ETF Module**: Exchange-traded fund specific attributes and relationships
- **Crypto Module**: Cryptocurrency market data and characteristics
- **Mutual Fund Module**: Mutual fund performance and composition data

4.3 Data Model

4.3.1 Core Entity Relationship Design

**Primary Entities and Relationships**

*Asset Management Entities*
```
Asset (Primary Entity)
├── asset_id (UUID, Primary Key)
├── symbol (String, Unique Index)
├── name (String)
├── asset_type (Enum: STOCK, ETF, MUTUAL_FUND, CRYPTO)
├── exchange (String)
├── sector (String)
├── industry (String)
├── market_cap (Decimal)
├── created_at (Timestamp)
└── updated_at (Timestamp)

Relationships:
- Asset 1:N DailyPrice (time-series price data)
- Asset 1:N AssetDailySignals (calculated indicators)
- Asset M:N NewsArticle (through NewsAssetMap)
- Asset 1:N QuarterlyFundamental (financial metrics)
```

*Time-Series Data Entities*
```
DailyPrice (TimescaleDB Hypertable)
├── price_id (UUID, Primary Key)
├── asset_id (UUID, Foreign Key → Asset)
├── date (Date, Hypertable Partition Key)
├── open_price (Decimal)
├── high_price (Decimal)
├── low_price (Decimal)
├── close_price (Decimal)
├── volume (BigInteger)
├── adjusted_close (Decimal)
├── created_at (Timestamp)

Indexes:
- Primary: (asset_id, date) for efficient time-range queries
- Covering: Include price fields to avoid table lookups
- Partial: Recent data (last 90 days) for high-frequency access
```

*Signal Generation Entities*
```
AssetDailySignals (Calculated Metrics)
├── signal_id (UUID, Primary Key)
├── asset_id (UUID, Foreign Key → Asset)
├── date (Date, Partition Key)
├── return_1d (Decimal)        # 1-day return
├── return_30d (Decimal)       # 30-day return
├── return_365d (Decimal)      # 365-day return
├── volatility_30d (Decimal)   # Annualized 30-day volatility
├── sma_20 (Decimal)          # 20-day simple moving average
├── sma_50 (Decimal)          # 50-day simple moving average
├── sma_200 (Decimal)         # 200-day simple moving average
├── avg_sentiment (Decimal)    # Aggregated sentiment score
├── news_count (Integer)       # Daily news article count
├── social_sentiment (Decimal) # Social media sentiment
├── calculated_at (Timestamp)

Constraints:
- Unique constraint on (asset_id, date)
- Check constraints for reasonable value ranges
- Not null constraints on critical calculation fields
```

4.3.2 News and Sentiment Data Model

**News Content Management**
```
NewsArticle (Content Storage)
├── article_id (UUID, Primary Key)
├── title (Text, Full-text Indexed)
├── content (Text, Full-text Indexed)
├── url (String, Unique)
├── source (String, Indexed)
├── author (String)
├── published_at (Timestamp, Indexed)
├── collected_at (Timestamp)
├── language (String, Default: 'en')
├── article_type (Enum: NEWS, ANALYSIS, EARNINGS, ANNOUNCEMENT)

Full-Text Search:
- PostgreSQL full-text search on title and content
- Weighted ranking: title (A), content (B)
- Custom dictionary for financial terms
```

**Sentiment Analysis Results**
```
NewsSentiment (Analysis Results)
├── sentiment_id (UUID, Primary Key)
├── article_id (UUID, Foreign Key → NewsArticle)
├── sentiment_score (Decimal, Range: -1.0 to 1.0)
├── confidence_score (Decimal, Range: 0.0 to 1.0)
├── model_version (String)      # FinBERT, VADER, etc.
├── positive_prob (Decimal)
├── negative_prob (Decimal)
├── neutral_prob (Decimal)
├── processed_at (Timestamp)

Indexes:
- article_id for article-to-sentiment lookup
- (sentiment_score, confidence_score) for filtering
- processed_at for temporal analysis
```

**Asset-News Relationship Mapping**
```
NewsAssetMap (Many-to-Many Relationship)
├── map_id (UUID, Primary Key)
├── article_id (UUID, Foreign Key → NewsArticle)
├── asset_id (UUID, Foreign Key → Asset)
├── relevance_score (Decimal, Range: 0.0 to 1.0)
├── extraction_method (Enum: SYMBOL_MATCH, NLP_ENTITY, MANUAL)
├── created_at (Timestamp)

Unique Constraint: (article_id, asset_id)
Indexes:
- article_id for article-based queries
- asset_id for asset-based news retrieval
- relevance_score for quality filtering
```

4.3.3 User and Portfolio Data Model

**User Management**
```
User (Authentication and Preferences)
├── user_id (UUID, Primary Key)
├── username (String, Unique)
├── email (String, Unique, Indexed)
├── password_hash (String)
├── first_name (String)
├── last_name (String)
├── risk_tolerance (Enum: CONSERVATIVE, MODERATE, AGGRESSIVE)
├── investment_horizon (Enum: SHORT, MEDIUM, LONG)
├── preferred_sectors (JSON Array)
├── excluded_assets (JSON Array)
├── created_at (Timestamp)
├── last_login (Timestamp)
├── is_active (Boolean, Default: True)

Privacy and Security:
- Email encryption for PII protection
- Password hashing with bcrypt
- Session management for authentication
```

**Portfolio Tracking (Future Enhancement)**
```
Portfolio (Investment Tracking)
├── portfolio_id (UUID, Primary Key)
├── user_id (UUID, Foreign Key → User)
├── portfolio_name (String)
├── total_value (Decimal)
├── cash_balance (Decimal)
├── created_at (Timestamp)
├── updated_at (Timestamp)

PortfolioHolding (Individual Positions)
├── holding_id (UUID, Primary Key)
├── portfolio_id (UUID, Foreign Key → Portfolio)
├── asset_id (UUID, Foreign Key → Asset)
├── quantity (Decimal)
├── average_cost (Decimal)
├── current_value (Decimal)
├── last_updated (Timestamp)
```

4.4 Process Flows

4.4.1 Data Collection and Processing Workflow

**Daily Market Data Collection Process**
```
1. Market Data Collection Initialization
   ├── Schedule: Daily at 6:00 PM EST (post-market close)
   ├── Trigger: APScheduler cron job
   └── Duration: Target 2 hours, Maximum 4 hours

2. Asset Universe Validation
   ├── Query active assets from Asset table
   ├── Validate data source availability
   ├── Generate collection task list (3,943+ assets)
   └── Initialize progress tracking

3. Price Data Collection
   ├── Parallel collection workers (5 concurrent)
   ├── Rate limiting compliance (API-specific limits)
   ├── Data validation and quality checks
   └── Temporary storage in staging tables

4. Data Quality Validation
   ├── Price reasonableness checks (outlier detection)
   ├── Volume validation against historical patterns
   ├── Missing data identification and handling
   └── Cross-source validation where available

5. Database Integration
   ├── Upsert operations into DailyPrice hypertable
   ├── Transaction management for data consistency
   ├── Conflict resolution for duplicate data
   └── Audit logging for traceability

6. Signal Generation Trigger
   ├── Initiate AssetDailySignals calculation
   ├── Technical indicator computation
   ├── Moving average updates
   └── Volatility and return calculations

7. Quality Assurance and Reporting
   ├── Data completeness validation (target >99%)
   ├── Processing performance metrics
   ├── Error reporting and alerting
   └── Success confirmation and next-day preparation
```

**News Collection and Sentiment Processing**
```
1. News Source Monitoring
   ├── Schedule: Every 15 minutes during market hours
   ├── Sources: Reuters, Bloomberg, Yahoo Finance, MarketWatch
   ├── Rate limiting: Adaptive based on API limits
   └── Deduplication: URL and content hash comparison

2. Content Processing Pipeline
   ├── Article extraction and normalization
   ├── Language detection and filtering (English priority)
   ├── Content quality assessment (minimum word count)
   └── Temporal relevance filtering (24-48 hour window)

3. Asset Relationship Extraction
   ├── Symbol recognition in title and content
   ├── Company name matching with fuzzy logic
   ├── NLP entity extraction for indirect references
   └── Relevance scoring (0.0-1.0 scale)

4. Sentiment Analysis Processing
   ├── FinBERT model inference for financial news
   ├── VADER analysis for social media content
   ├── Ensemble scoring for improved accuracy
   └── Confidence assessment and quality filtering

5. Database Integration and Indexing
   ├── NewsArticle insertion with full-text indexing
   ├── NewsSentiment results storage
   ├── NewsAssetMap relationship creation
   └── Search index updates for real-time queries

6. Signal Integration
   ├── Daily sentiment aggregation by asset
   ├── AssetDailySignals sentiment field updates
   ├── Historical sentiment trend calculation
   └── Recommendation engine notification
```

4.4.2 Recommendation Generation Workflow

**User Investment Recommendation Process**
```
1. User Request Processing
   ├── Input: Capital amount, risk tolerance, investment horizon
   ├── Validation: Parameter ranges and consistency checks
   ├── Authentication: User session validation
   └── Request logging for analytics

2. Asset Universe Filtering
   ├── Exclude user-specified restricted assets
   ├── Apply sector preferences if specified
   ├── Minimum liquidity requirements (volume thresholds)
   └── Data quality filtering (recent price data required)

3. Signal Aggregation and Scoring
   ├── Retrieve latest AssetDailySignals for filtered universe
   ├── Technical indicator normalization and weighting
   ├── Sentiment score integration with confidence weighting
   └── Composite score calculation per asset

4. Risk Assessment and Portfolio Construction
   ├── Volatility-based risk categorization
   ├── Correlation analysis for diversification
   ├── User risk tolerance alignment
   └── Position sizing based on Modern Portfolio Theory

5. Recommendation Ranking and Selection
   ├── Risk-adjusted return scoring (Sharpe ratio approximation)
   ├── Diversification optimization across sectors/asset types
   ├── Capital allocation optimization
   └── Top-N selection based on user capital constraints

6. Response Generation and Delivery
   ├── Recommendation formatting with explanations
   ├── Risk metrics calculation and presentation
   ├── Performance projection and backtesting data
   └── JSON response with comprehensive metadata

7. Analytics and Feedback Collection
   ├── User interaction logging for recommendation improvement
   ├── Performance tracking for algorithm optimization
   ├── A/B testing data collection
   └── User satisfaction metrics collection
```

4.4.3 Real-Time Data Processing (Future Implementation)

**Streaming Data Architecture**
```
1. Real-Time Price Feed Integration
   ├── WebSocket connections to market data providers
   ├── Message queuing for high-frequency updates
   ├── Stream processing for immediate signal updates
   └── Real-time dashboard updates

2. News and Social Media Streaming
   ├── Twitter/X streaming API integration
   ├── Reddit real-time comment monitoring
   ├── Immediate sentiment analysis processing
   └── Event-driven recommendation updates

3. Alert and Notification System
   ├── Significant price movement detection
   ├── Breaking news impact assessment
   ├── User-specific alert criteria evaluation
   └── Multi-channel notification delivery (web, mobile, email)
```

4.5 Interaction Sequences

4.5.1 User Recommendation Request Sequence

**Primary User Flow: Investment Recommendation Generation**
```
User → Streamlit Frontend → FastAPI Backend → Database → Response Flow

1. User Interface Interaction
   User: Accesses Streamlit dashboard
   Frontend: Renders investment preference form
   User: Inputs capital ($25,000), risk (Moderate), horizon (5 years)
   Frontend: Validates input ranges and completeness

2. API Request Processing
   Frontend → API: POST /api/v1/recommend
   Request Body: {
     "capital": 25000,
     "risk_tolerance": "moderate", 
     "investment_horizon": "medium",
     "excluded_sectors": [],
     "user_preferences": {...}
   }

3. Authentication and Validation
   API Gateway: Session authentication validation
   Request Validator: Pydantic model validation
   Rate Limiter: Request frequency check
   Logger: Request logging with user context

4. Business Logic Processing
   Recommendation Service: Initialize recommendation engine
   Asset Filter: Query eligible assets from database
   Signal Aggregator: Retrieve latest AssetDailySignals
   Risk Assessor: Calculate portfolio risk metrics

5. Database Query Sequence
   DB Connection Pool: Acquire connection from pool
   Query 1: SELECT eligible assets WHERE market_cap > threshold
   Query 2: SELECT latest signals WHERE asset_id IN (asset_list)
   Query 3: SELECT sentiment data WHERE date >= (today - 30)
   Connection Pool: Return connection to pool

6. Algorithm Processing
   Signal Processor: Normalize and weight technical indicators
   Sentiment Integrator: Combine news and social sentiment
   Portfolio Optimizer: Apply Modern Portfolio Theory
   Risk Calculator: Compute risk-adjusted returns

7. Response Generation
   Recommendation Formatter: Structure recommendation response
   Explanation Generator: Create human-readable rationales
   Risk Metrics: Calculate portfolio beta, volatility, VaR
   Performance Projector: Historical backtesting results

8. API Response Delivery
   Response Validator: Ensure response schema compliance
   Cache Manager: Store response for potential replay
   API Gateway → Frontend: HTTP 200 with recommendation JSON
   Response Time Target: <2 seconds end-to-end

9. Frontend Display Processing
   Data Transformer: Convert API response to UI components
   Chart Generator: Create portfolio allocation visualizations
   Risk Display: Format risk metrics for user understanding
   Action Buttons: Enable user interaction (accept, modify, save)

10. User Feedback Collection
    User: Reviews recommendations and provides feedback
    Frontend: Captures user interactions and satisfaction rating
    Analytics Service: Logs interaction data for algorithm improvement
    Database: Stores user feedback for future enhancements
```

4.5.2 Data Collection and Signal Generation Sequence

**Automated Daily Data Processing Workflow**
```
Scheduler → Data Collectors → Signal Processors → Database Updates

1. Scheduled Task Initialization
   APScheduler: Triggers daily data collection at 6:00 PM EST
   Task Manager: Initializes collection workflow
   Logger: Records task start and expected completion time
   Resource Monitor: Validates system resources availability

2. Market Data Collection Coordination
   Master Collector: Queries Asset table for active symbols
   Rate Limiter: Calculates API request schedule within limits
   Worker Pool: Initializes parallel collection workers (5 concurrent)
   Progress Tracker: Sets up completion monitoring

3. Individual Asset Data Collection
   For Each Asset (3,943+ iterations):
   ├── Data Provider API: Request OHLCV data for date range
   ├── Response Validator: Validate data quality and completeness
   ├── Data Transformer: Normalize data format and types
   └── Staging Storage: Temporary storage for batch processing

4. Data Quality Validation Process
   Validator Service: Batch validation of collected data
   ├── Outlier Detection: Statistical analysis for price anomalies
   ├── Volume Validation: Historical pattern comparison
   ├── Missing Data Handler: Interpolation or forward-fill strategies
   └── Quality Metrics: Calculate data completeness percentage

5. Database Integration Transaction
   Transaction Begin: Start database transaction for data consistency
   ├── Batch Insert: Upsert DailyPrice records using TimescaleDB
   ├── Conflict Resolution: Handle duplicate data gracefully
   ├── Index Maintenance: Update relevant indexes automatically
   └── Transaction Commit: Ensure atomic data integration

6. Signal Generation Trigger Sequence
   Signal Processor: Initialize calculation engine
   ├── Asset Iteration: Process each asset with new price data
   ├── Technical Indicators: Calculate SMA, volatility, returns
   ├── Sentiment Integration: Merge with news sentiment data
   └── Database Update: Upsert AssetDailySignals records

7. Validation and Completion
   Quality Assurance: Validate signal generation completeness
   ├── Coverage Check: Ensure signals for >99% of assets
   ├── Calculation Validation: Spot-check indicator calculations
   ├── Performance Metrics: Log processing time and resource usage
   └── Success Notification: Confirm task completion

8. Error Handling and Recovery
   Error Monitor: Continuous error detection throughout process
   ├── Retry Logic: Automatic retry for transient failures
   ├── Dead Letter Queue: Failed items for manual investigation
   ├── Alert System: Notification for critical failures
   └── Rollback Procedures: Data consistency recovery mechanisms
```

4.6 API Principles

4.6.1 REST API Design Standards

**Resource-Oriented Architecture**
The Lumia API follows REST principles with clear resource identification and standard HTTP methods:

```
API Endpoint Structure:
/api/v1/{resource}/{id?}/{sub-resource?}

Primary Resources:
- /api/v1/assets                 # Asset management
- /api/v1/recommendations        # Investment recommendations  
- /api/v1/signals               # Technical and sentiment signals
- /api/v1/news                  # News articles and sentiment
- /api/v1/users                 # User management (future)
- /api/v1/portfolios            # Portfolio tracking (future)

HTTP Method Usage:
- GET: Resource retrieval and querying
- POST: Resource creation and complex operations
- PUT: Complete resource replacement
- PATCH: Partial resource updates
- DELETE: Resource removal
```

**Request/Response Standards**
```
Standard Request Headers:
- Content-Type: application/json
- Accept: application/json
- Authorization: Bearer {jwt_token} (future authentication)
- X-Request-ID: {uuid} for request tracing

Standard Response Format:
{
  "success": boolean,
  "data": object | array,
  "message": string,
  "metadata": {
    "timestamp": "ISO-8601",
    "request_id": "uuid",
    "execution_time_ms": number,
    "api_version": "v1"
  },
  "pagination": {           # For paginated responses
    "page": number,
    "per_page": number, 
    "total_pages": number,
    "total_items": number
  }
}

Error Response Format:
{
  "success": false,
  "error": {
    "code": "ERROR_CODE",
    "message": "Human readable message",
    "details": object,
    "timestamp": "ISO-8601"
  }
}
```

4.6.2 API Endpoint Specifications

**Investment Recommendation Endpoint**
```
POST /api/v1/recommendations
Description: Generate personalized investment recommendations

Request Schema:
{
  "capital": number,              # Available investment capital
  "risk_tolerance": "conservative" | "moderate" | "aggressive",
  "investment_horizon": "short" | "medium" | "long",
  "preferred_sectors": string[],   # Optional sector preferences
  "excluded_assets": string[],     # Assets to exclude from recommendations  
  "diversification_preference": "high" | "medium" | "low",
  "esg_preference": boolean        # ESG screening preference
}

Response Schema:
{
  "success": true,
  "data": {
    "recommendations": [
      {
        "asset": {
          "symbol": "AAPL",
          "name": "Apple Inc.",
          "asset_type": "stock",
          "sector": "Technology"
        },
        "allocation": {
          "percentage": 15.5,
          "amount": 3875.00,
          "shares": 25
        },
        "rationale": {
          "technical_score": 0.78,
          "sentiment_score": 0.65,
          "fundamental_score": 0.82,
          "overall_score": 0.75
        },
        "risk_metrics": {
          "beta": 1.21,
          "volatility": 0.24,
          "sharpe_ratio": 1.45
        }
      }
    ],
    "portfolio_summary": {
      "total_allocation": 25000.00,
      "expected_return": 0.089,
      "portfolio_risk": 0.156,
      "sharpe_ratio": 0.57,
      "diversification_score": 0.78
    }
  }
}
```

**Asset Search and Information Endpoint**
```
GET /api/v1/assets
Description: Search and retrieve asset information

Query Parameters:
- symbol: Asset symbol filter (e.g., "AAPL")
- asset_type: Filter by type (stock, etf, mutual_fund, crypto)
- sector: Sector filter
- min_market_cap: Minimum market capitalization
- max_market_cap: Maximum market capitalization
- search: Full-text search on name and description
- page: Page number for pagination (default: 1)
- per_page: Items per page (default: 50, max: 200)

Response Schema:
{
  "success": true,
  "data": [
    {
      "asset_id": "uuid",
      "symbol": "AAPL", 
      "name": "Apple Inc.",
      "asset_type": "stock",
      "exchange": "NASDAQ",
      "sector": "Technology",
      "industry": "Consumer Electronics",
      "market_cap": 2800000000000,
      "latest_price": 175.84,
      "daily_change": 2.15,
      "daily_change_percent": 1.24
    }
  ],
  "pagination": {...}
}
```

**Signal and Analytics Endpoint**
```
GET /api/v1/signals/{asset_id}
Description: Retrieve technical and sentiment signals for specific asset

Query Parameters:
- start_date: Start date for signal history (ISO-8601)
- end_date: End date for signal history (ISO-8601)  
- indicators: Comma-separated list of indicators to include

Response Schema:
{
  "success": true,
  "data": {
    "asset_info": {
      "symbol": "AAPL",
      "name": "Apple Inc."
    },
    "latest_signals": {
      "date": "2025-09-29",
      "return_30d": 0.045,
      "volatility_30d": 0.28,
      "sma_20": 172.35,
      "sma_50": 168.92,
      "sma_200": 165.44,
      "sentiment_score": 0.65,
      "news_count": 12
    },
    "historical_signals": [
      {
        "date": "2025-09-28",
        "return_30d": 0.042,
        "volatility_30d": 0.29,
        // ... additional indicators
      }
    ]
  }
}
```

4.6.3 API Security and Performance

**Authentication and Authorization**
```
Authentication Strategy (Planned):
- JWT (JSON Web Tokens) for stateless authentication
- OAuth 2.0 integration for third-party authentication
- API key authentication for programmatic access
- Session-based authentication for web application

Authorization Levels:
- Public: Basic asset information and market data
- Authenticated: Personalized recommendations and portfolio tracking
- Premium: Advanced analytics and real-time data access
- Admin: System management and user administration
```

**Rate Limiting and Performance**
```
Rate Limiting Strategy:
- Anonymous users: 100 requests/hour
- Authenticated users: 1000 requests/hour  
- Premium users: 10000 requests/hour
- Sliding window implementation with Redis

Performance Optimization:
- Response caching for frequently requested data
- Database connection pooling (10-50 connections)
- Query result caching with 5-minute TTL
- Async request processing with FastAPI

Monitoring and Analytics:
- Request/response logging with structured format
- Performance metrics collection (response times, error rates)
- API usage analytics for optimization insights
- Health check endpoints for system monitoring
```

4.7 Deployment Context

4.7.1 Development Environment Architecture

**Local Development Setup**
```
Development Environment Components:
├── Python 3.10 Virtual Environment
├── PostgreSQL 14+ with TimescaleDB extension
├── FastAPI development server (uvicorn)
├── Streamlit development server
├── APScheduler for background tasks
└── Local data collection and testing tools

Directory Structure:
/Lumia/
├── app/                    # Frontend Streamlit application
├── models/                 # Database models and schemas
├── collectors/             # Data collection modules
├── services/               # Business logic services
├── scripts/                # Utility and maintenance scripts
├── tests/                  # Unit and integration tests
├── alembic/               # Database migration management
├── logs/                  # Application logging output
└── requirements.txt       # Python dependency specification

Development Workflow:
1. Virtual environment activation
2. Database schema migration (alembic upgrade head)
3. Development server startup (uvicorn and streamlit)
4. Local testing with sample data
5. Code changes and hot reload testing
```

**Database Development Configuration**
```
PostgreSQL Development Settings:
- Database: lumia_dev
- TimescaleDB Extension: Enabled
- Connection Pool: 5-10 connections
- Logging: All queries for debugging
- Backup Strategy: Daily local snapshots

Alembic Migration Management:
- Migration Environment: development
- Auto-generation: Enabled for model changes
- Version Control: Git integration for migration files
- Rollback Support: Full up/down migration capability
```

4.7.2 Staging Environment Architecture

**Pre-Production Environment**
```
Staging Environment Purpose:
- Production-like testing environment
- Performance validation and load testing
- Integration testing with external APIs
- User acceptance testing platform
- Deployment procedure validation

Infrastructure Components:
├── Application Server: Linux VM (Ubuntu 22.04 LTS)
├── Database Server: PostgreSQL 14 with TimescaleDB
├── Web Server: Nginx reverse proxy
├── Process Manager: Systemd for service management
├── Monitoring: Application and system metrics collection
└── Backup System: Automated database and file backups

Deployment Architecture:
Internet → Nginx → FastAPI (uvicorn) → PostgreSQL/TimescaleDB
              ↓
         Streamlit → FastAPI (same instance)
```

4.7.3 Production Deployment Strategy

**Cloud-Ready Architecture Design**
```
Production Deployment Options:

Option 1: Single Server Deployment (Current)
├── VPS/Cloud Instance (4 CPU, 16GB RAM, 200GB SSD)
├── Docker containerization for application isolation
├── Nginx for reverse proxy and static file serving
├── PostgreSQL with TimescaleDB on same instance
└── Automated backup to cloud storage

Option 2: Microservices Cloud Deployment (Future)
├── Container Orchestration: Kubernetes or Docker Swarm
├── API Services: Multiple FastAPI instances with load balancing
├── Database: Managed PostgreSQL with TimescaleDB (AWS RDS, GCP Cloud SQL)
├── Frontend: Streamlit or React deployment with CDN
├── Message Queue: Redis/RabbitMQ for async processing
└── Monitoring: Prometheus, Grafana, and ELK stack

Scalability Considerations:
- Horizontal scaling: Load balancer with multiple API instances
- Database scaling: Read replicas for analytical queries
- Caching layer: Redis for frequently accessed data
- CDN integration: Static asset delivery optimization
```

**Container Deployment Configuration**
```
Docker Architecture:

Dockerfile.api (FastAPI Backend):
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

Dockerfile.frontend (Streamlit):
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install streamlit
COPY app/ .
EXPOSE 8501
CMD ["streamlit", "run", "main.py", "--server.port=8501"]

Docker Compose Configuration:
version: '3.8'
services:
  database:
    image: timescale/timescaledb:latest-pg14
    environment:
      POSTGRES_DB: lumia_prod
      POSTGRES_USER: lumia_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    depends_on:
      - database
    environment:
      DATABASE_URL: postgresql://lumia_user:${DB_PASSWORD}@database/lumia_prod
    ports:
      - "8000:8000"

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    depends_on:
      - api
    ports:
      - "8501:8501"

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - api
      - frontend
```

4.8 Configuration Management

4.8.1 Environment Configuration Strategy

**Configuration Hierarchy**
```
Configuration Management Architecture:
├── Environment Variables (.env files)
├── Application Config Files (config.py)
├── Database Configuration (alembic.ini)
├── Deployment Configuration (docker-compose.yml)
└── Infrastructure Configuration (nginx.conf, systemd services)

Environment-Specific Configuration:
.env.development    # Local development settings
.env.staging       # Staging environment settings  
.env.production    # Production environment settings
.env.sample        # Template with required variables
```

**Application Configuration Structure**
```python
# config.py - Central Configuration Management
import os
from typing import Optional
from pydantic import BaseSettings, validator

class Settings(BaseSettings):
    # Database Configuration
    database_url: str
    database_pool_size: int = 10
    database_max_overflow: int = 20
    
    # API Configuration  
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_workers: int = 4
    debug_mode: bool = False
    
    # External API Keys
    news_api_key: Optional[str] = None
    twitter_api_key: Optional[str] = None
    reddit_api_key: Optional[str] = None
    
    # Security Configuration
    secret_key: str
    jwt_expiration_hours: int = 24
    cors_origins: list = ["http://localhost:8501"]
    
    # Data Collection Settings
    collection_schedule_hour: int = 18  # 6 PM EST
    max_collection_workers: int = 5
    api_rate_limit_per_minute: int = 60
    
    # Signal Generation Parameters
    volatility_window_days: int = 30
    return_calculation_periods: list = [1, 30, 365]
    moving_average_periods: list = [20, 50, 200]
    
    # Recommendation Engine Settings
    max_recommendations: int = 10
    min_asset_market_cap: float = 1e9  # $1B minimum
    default_diversification_target: float = 0.8
    
    class Config:
        env_file = ".env"
        case_sensitive = False
    
    @validator("database_url")
    def validate_database_url(cls, v):
        if not v.startswith(("postgresql://", "postgresql+asyncpg://")):
            raise ValueError("Database URL must use PostgreSQL")
        return v

# Global settings instance
settings = Settings()
```

4.8.2 Database Configuration Management

**Alembic Migration Configuration**
```ini
# alembic.ini - Database Migration Configuration
[alembic]
script_location = alembic
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = postgresql://%(DB_USER)s:%(DB_PASSWORD)s@%(DB_HOST)s/%(DB_NAME)s

[post_write_hooks]
hooks = black
black.type = console_scripts
black.entrypoint = black
black.options = -l 88

[loggers]
keys = root,sqlalchemy,alembic

[handlers] 
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**Environment-Specific Database Settings**
```bash
# .env.development
DATABASE_URL=postgresql://lumia_user:dev_password@localhost:5432/lumia_dev
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
DEBUG_MODE=true
LOG_LEVEL=DEBUG

# .env.production  
DATABASE_URL=postgresql://lumia_user:${PROD_PASSWORD}@prod-db:5432/lumia_prod
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=50
DEBUG_MODE=false
LOG_LEVEL=INFO
SSL_REQUIRE=true
```

4.8.3 Logging and Monitoring Configuration

**Structured Logging Configuration**
```python
# logging_config.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        if hasattr(record, 'user_id'):
            log_entry['user_id'] = record.user_id
        if hasattr(record, 'request_id'):
            log_entry['request_id'] = record.request_id
            
        return json.dumps(log_entry)

# Logging configuration
LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'json': {
            '()': JSONFormatter
        },
        'standard': {
            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'standard',
            'level': 'INFO'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'logs/lumia.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'json',
            'level': 'INFO'
        }
    },
    'loggers': {
        'lumia': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False
        },
        'sqlalchemy.engine': {
            'handlers': ['file'],
            'level': 'WARNING',
            'propagate': False
        }
    }
}
```

**Monitoring and Health Check Configuration**
```python
# monitoring.py - Application Health Monitoring
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
import psutil

router = APIRouter(prefix="/health", tags=["monitoring"])

@router.get("/")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "v0.0.1"
    }

@router.get("/detailed")
async def detailed_health_check(db: Session = Depends(get_db)):
    try:
        # Database connectivity test
        db.execute("SELECT 1")
        db_status = "healthy"
    except Exception as e:
        db_status = f"unhealthy: {str(e)}"
    
    # System resource monitoring
    system_info = {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_percent": psutil.disk_usage('/').percent
    }
    
    # Data freshness check
    latest_price_query = """
        SELECT MAX(date) as latest_date 
        FROM daily_price 
        WHERE date >= CURRENT_DATE - INTERVAL '7 days'
    """
    latest_data = db.execute(latest_price_query).fetchone()
    
    return {
        "status": "healthy" if db_status == "healthy" else "degraded",
        "timestamp": datetime.utcnow().isoformat(),
        "components": {
            "database": db_status,
            "system_resources": system_info,
            "data_freshness": {
                "latest_price_date": latest_data.latest_date if latest_data else None
            }
        }
    }
```

---

*This comprehensive system design provides the architectural foundation for building, deploying, and maintaining the Lumia AI Financial Analytics Platform. The design emphasizes scalability, maintainability, and performance while accommodating both current requirements and future enhancements including real-time processing, advanced machine learning integration, and mobile application support.*